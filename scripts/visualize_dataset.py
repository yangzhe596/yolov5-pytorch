#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯è§†åŒ–ä¿®å¤åçš„æ•°æ®é›†
éªŒè¯æ—¶é—´æˆ³å¯¹åº”å…³ç³»æ˜¯å¦æ­£ç¡®
"""

import os
import cv2
import json
from pathlib import Path
import argparse


def visualize_coco_sample(coco_root, modality='rgb', split='train', num_samples=5):
    """å¯è§†åŒ–COCOæ•°æ®é›†æ ·æœ¬"""
    
    print("=" * 80)
    print(f"å¯è§†åŒ–COCOæ•°æ®é›† - {modality.upper()} - {split}")
    print("=" * 80)
    
    # è¯»å–COCOæ ‡æ³¨
    anno_file = Path(coco_root) / modality / 'annotations' / f'instances_{split}.json'
    
    if not anno_file.exists():
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {anno_file}")
        return
    
    with open(anno_file, 'r') as f:
        coco_data = json.load(f)
    
    images = coco_data['images']
    annotations = coco_data['annotations']
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   å›¾ç‰‡æ•°: {len(images)}")
    print(f"   æ ‡æ³¨æ•°: {len(annotations)}")
    
    # åˆ›å»ºimage_idåˆ°annotationsçš„æ˜ å°„
    anno_dict = {}
    for anno in annotations:
        img_id = anno['image_id']
        if img_id not in anno_dict:
            anno_dict[img_id] = []
        anno_dict[img_id].append(anno)
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    import random
    random.seed(42)
    sample_images = random.sample(images, min(num_samples, len(images)))
    
    print(f"\nğŸ–¼ï¸  å¯è§†åŒ– {len(sample_images)} ä¸ªæ ·æœ¬:")
    
    output_dir = Path('visualization_fixed')
    output_dir.mkdir(exist_ok=True)
    
    for i, img_info in enumerate(sample_images):
        img_id = img_info['id']
        filename = img_info['file_name']
        img_path = Path(coco_root) / modality / split / filename
        
        if not img_path.exists():
            print(f"   âš ï¸  å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
            continue
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(str(img_path))
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        if img_id in anno_dict:
            for anno in anno_dict[img_id]:
                bbox = anno['bbox']  # [x, y, width, height]
                x, y, w, h = bbox
                
                # ç»˜åˆ¶çŸ©å½¢
                cv2.rectangle(img, 
                            (int(x), int(y)), 
                            (int(x + w), int(y + h)), 
                            (0, 255, 0), 2)
                
                # æ·»åŠ æ ‡ç­¾
                label = f"ID:{img_id}"
                cv2.putText(img, label, 
                          (int(x), int(y) - 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 
                          0.5, (0, 255, 0), 2)
        
        # æ·»åŠ æ—¶é—´æˆ³ä¿¡æ¯
        rel_time = img_info.get('relative_timestamp', 0)
        abs_time = img_info.get('absolute_timestamp', 0)
        
        info_text = f"Rel: {rel_time:.3f}s, Abs: {abs_time:.3f}s"
        cv2.putText(img, info_text,
                   (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   0.7, (255, 255, 0), 2)
        
        # ä¿å­˜
        output_path = output_dir / f"{modality}_{split}_{i+1}_{filename}"
        cv2.imwrite(str(output_path), img)
        
        print(f"   {i+1}. {filename}")
        print(f"      ç›¸å¯¹æ—¶é—´: {rel_time:.6f}s")
        print(f"      ç»å¯¹æ—¶é—´: {abs_time:.6f}s")
        print(f"      æ ‡æ³¨æ•°: {len(anno_dict.get(img_id, []))}")
        print(f"      ä¿å­˜è‡³: {output_path}")
    
    print(f"\nâœ… å¯è§†åŒ–å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 80)


def compare_with_original(video_id=3, sample_filename=None):
    """å¯¹æ¯”åŸå§‹æ•°æ®å’Œè½¬æ¢åçš„æ•°æ®"""
    
    print("\n" + "=" * 80)
    print(f"å¯¹æ¯”åŸå§‹æ•°æ®å’Œè½¬æ¢åçš„æ•°æ®")
    print("=" * 80)
    
    # åŸå§‹æ•°æ®è·¯å¾„
    seq_dir = Path(f'/mnt/data/datasets/fred/{video_id}')
    coord_file = seq_dir / 'coordinates.txt'
    rgb_dir = seq_dir / 'PADDED_RGB'
    
    # è¯»å–coordinates.txt
    annotations = {}
    with open(coord_file, 'r') as f:
        for line in f:
            if ':' in line:
                parts = line.strip().split(':')
                ts = float(parts[0])
                coords_str = parts[1].strip()
                coords = [float(x.strip()) for x in coords_str.split(',')]
                if len(coords) == 4:
                    annotations[ts] = coords
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ ·æœ¬ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ ‡æ³¨çš„å›¾ç‰‡
    if sample_filename is None:
        # è·å–æ‰€æœ‰RGBå›¾ç‰‡
        rgb_images = sorted([f for f in rgb_dir.iterdir() if f.suffix == '.jpg'])
        first_image = rgb_images[0]
        
        # è®¡ç®—ç¬¬ä¸€å¼ å›¾ç‰‡çš„ç»å¯¹æ—¶é—´
        parts = first_image.name.replace('.jpg', '').split('_')
        hours = int(parts[2])
        minutes = int(parts[3])
        seconds = float(parts[4])
        first_abs_time = hours * 3600 + minutes * 60 + seconds
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡æ³¨å¯¹åº”çš„å›¾ç‰‡
        first_anno_time = min(annotations.keys())
        target_abs_time = first_abs_time + first_anno_time
        
        # è½¬æ¢ä¸ºæ—¶:åˆ†:ç§’
        hours = int(target_abs_time // 3600)
        minutes = int((target_abs_time % 3600) // 60)
        seconds = target_abs_time % 60
        
        # æŸ¥æ‰¾æœ€æ¥è¿‘çš„å›¾ç‰‡
        target_pattern = f"Video_{video_id}_{hours:02d}_{minutes:02d}_{seconds:09.6f}"
        
        closest_image = None
        min_diff = float('inf')
        
        for img in rgb_images:
            img_parts = img.name.replace('.jpg', '').split('_')
            img_h = int(img_parts[2])
            img_m = int(img_parts[3])
            img_s = float(img_parts[4])
            img_abs = img_h * 3600 + img_m * 60 + img_s
            
            diff = abs(img_abs - target_abs_time)
            if diff < min_diff:
                min_diff = diff
                closest_image = img.name
        
        sample_filename = closest_image
    
    print(f"\nğŸ“‹ æ ·æœ¬: {sample_filename}")
    
    # è¯»å–åŸå§‹å›¾ç‰‡
    img_path = rgb_dir / sample_filename
    img = cv2.imread(str(img_path))
    
    # æå–æ—¶é—´æˆ³
    parts = sample_filename.replace('.jpg', '').split('_')
    hours = int(parts[2])
    minutes = int(parts[3])
    seconds = float(parts[4])
    abs_time = hours * 3600 + minutes * 60 + seconds
    
    # è®¡ç®—ç›¸å¯¹æ—¶é—´
    rgb_images = sorted([f for f in rgb_dir.iterdir() if f.suffix == '.jpg'])
    first_image = rgb_images[0]
    first_parts = first_image.name.replace('.jpg', '').split('_')
    first_abs = int(first_parts[2]) * 3600 + int(first_parts[3]) * 60 + float(first_parts[4])
    
    rel_time = abs_time - first_abs
    
    print(f"   ç»å¯¹æ—¶é—´: {abs_time:.6f}s ({hours:02d}:{minutes:02d}:{seconds:09.6f})")
    print(f"   ç›¸å¯¹æ—¶é—´: {rel_time:.6f}s")
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ ‡æ³¨
    closest_anno_time = min(annotations.keys(), key=lambda t: abs(t - rel_time))
    time_diff = abs(closest_anno_time - rel_time)
    bbox = annotations[closest_anno_time]
    
    print(f"   æœ€è¿‘æ ‡æ³¨æ—¶é—´: {closest_anno_time:.6f}s")
    print(f"   æ—¶é—´å·®: {time_diff:.6f}s")
    print(f"   è¾¹ç•Œæ¡†: {bbox}")
    
    # ç»˜åˆ¶è¾¹ç•Œæ¡†
    x1, y1, x2, y2 = bbox
    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
    
    # æ·»åŠ ä¿¡æ¯
    info_text = f"Rel: {rel_time:.3f}s, Anno: {closest_anno_time:.3f}s, Diff: {time_diff:.3f}s"
    cv2.putText(img, info_text, (10, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
    
    # ä¿å­˜
    output_dir = Path('visualization_fixed')
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"original_{video_id}_{sample_filename}"
    cv2.imwrite(str(output_path), img)
    
    print(f"   ä¿å­˜è‡³: {output_path}")
    
    if time_diff <= 0.05:
        print(f"\nâœ… æ—¶é—´æˆ³åŒ¹é…æ­£ç¡®ï¼")
    else:
        print(f"\nâš ï¸  æ—¶é—´æˆ³åŒ¹é…å¯èƒ½æœ‰é—®é¢˜ï¼Œæ—¶é—´å·® {time_diff:.6f}s > 0.05s")
    
    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–ä¿®å¤åçš„æ•°æ®é›†')
    parser.add_argument('--coco_root', type=str, default='datasets/fred_coco',
                       help='COCOæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--modality', type=str, default='rgb',
                       choices=['rgb', 'event'],
                       help='æ¨¡æ€')
    parser.add_argument('--split', type=str, default='train',
                       choices=['train', 'val', 'test'],
                       help='æ•°æ®é›†åˆ’åˆ†')
    parser.add_argument('--num_samples', type=int, default=5,
                       help='å¯è§†åŒ–æ ·æœ¬æ•°')
    parser.add_argument('--compare_original', action='store_true',
                       help='å¯¹æ¯”åŸå§‹æ•°æ®')
    parser.add_argument('--video_id', type=int, default=3,
                       help='è§†é¢‘åºåˆ—IDï¼ˆç”¨äºå¯¹æ¯”åŸå§‹æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    if args.compare_original:
        compare_with_original(args.video_id)
    else:
        visualize_coco_sample(args.coco_root, args.modality, args.split, args.num_samples)
