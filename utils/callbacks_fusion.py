"""
Fusion æ¨¡å‹ä¸“ç”¨çš„è¯„ä¼°å›è°ƒ
æ”¯æŒåŒæ¨¡æ€è¾“å…¥ï¼ˆRGB + Eventï¼‰è¿›è¡Œ mAP è¯„ä¼°
"""
import os
import json
import shutil
import datetime

import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from .callbacks_coco import CocoEvalCallback
from .utils_bbox import DecodeBox
from .utils import cvtColor, preprocess_input, resize_image


class FusionCocoEvalCallback(CocoEvalCallback):
    """
    Fusion æ¨¡å‹ä¸“ç”¨çš„ COCO è¯„ä¼°å›è°ƒ
    æ”¯æŒå¤šç§è¯„ä¼°æ¨¡å¼ï¼š
    - rgb_only: åªä½¿ç”¨ RGB æ¨¡æ€è¯„ä¼°
    - event_only: åªä½¿ç”¨ Event æ¨¡æ€è¯„ä¼°
    - dual_avg: ä½¿ç”¨åŒæ¨¡æ€å¹³å‡å€¼è¯„ä¼°
    """
    
    def __init__(self, net, input_shape, anchors, anchors_mask, class_names, num_classes, 
                 coco_json_path, image_dir_rgb, image_dir_event, log_dir, cuda,
                 map_out_path=".temp_map_out", max_boxes=100, confidence=0.05, 
                 nms_iou=0.5, letterbox_image=True, MINOVERLAP=0.5, 
                 eval_flag=True, period=1, max_eval_samples=None,
                 fusion_mode="rgb_only"):
        """
        Fusion æ¨¡å‹è¯„ä¼°å›è°ƒåˆå§‹åŒ–
        
        Args:
            net: Fusion æ¨¡å‹
            input_shape: è¾“å…¥å°ºå¯¸ [H, W]
            anchors: å…ˆéªŒæ¡†
            anchors_mask: å…ˆéªŒæ¡† mask
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            num_classes: ç±»åˆ«æ•°é‡
            coco_json_path: COCO æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„
            image_dir_rgb: RGB å›¾ç‰‡ç›®å½•
            image_dir_event: Event å›¾ç‰‡ç›®å½•
            log_dir: æ—¥å¿—ç›®å½•
            cuda: æ˜¯å¦ä½¿ç”¨ CUDA
            map_out_path: mAP è®¡ç®—ä¸´æ—¶ç›®å½•
            max_boxes: æœ€å¤§æ£€æµ‹æ¡†æ•°é‡
            confidence: ç½®ä¿¡åº¦é˜ˆå€¼
            nms_iou: NMS çš„ IOU é˜ˆå€¼
            letterbox_image: æ˜¯å¦ä½¿ç”¨ letterbox
            MINOVERLAP: mAP è®¡ç®—çš„ IOU é˜ˆå€¼
            eval_flag: æ˜¯å¦è¿›è¡Œè¯„ä¼°
            period: è¯„ä¼°å‘¨æœŸï¼ˆæ¯å¤šå°‘ä¸ª epoch è¯„ä¼°ä¸€æ¬¡ï¼‰
            max_eval_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰
            fusion_mode: Fusion è¯„ä¼°æ¨¡å¼
                - "rgb_only": åªä½¿ç”¨ RGB æ¨¡æ€ï¼ˆé»˜è®¤ï¼Œé€Ÿåº¦æœ€å¿«ï¼‰
                - "event_only": åªä½¿ç”¨ Event æ¨¡æ€
                - "dual_avg": ä½¿ç”¨åŒæ¨¡æ€å¹³å‡å€¼ï¼ˆéœ€è¦æ”¯æŒï¼‰
                - "dual_concat": æ‹¼æ¥åŒæ¨¡æ€ï¼ˆéœ€è¦æ”¯æŒï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆéœ€è¦ä¿®æ”¹ä¼ å‚ä»¥åŒ¹é…çˆ¶ç±»ï¼‰
        super().__init__(
            net=net,
            input_shape=input_shape,
            anchors=anchors,
            anchors_mask=anchors_mask,
            class_names=class_names,
            num_classes=num_classes,
            coco_json_path=coco_json_path,
            image_dir=image_dir_rgb,  # çˆ¶ç±»åªéœ€è¦ä¸€ä¸ª image_dir
            log_dir=log_dir,
            cuda=cuda,
            map_out_path=map_out_path,
            max_boxes=max_boxes,
            confidence=confidence,
            nms_iou=nms_iou,
            letterbox_image=letterbox_image,
            MINOVERLAP=MINOVERLAP,
            eval_flag=eval_flag,
            period=period,
            max_eval_samples=max_eval_samples
        )
        
        # Fusion ç‰¹æœ‰å‚æ•°
        self.image_dir_rgb = image_dir_rgb
        self.image_dir_event = image_dir_event
        self.fusion_mode = fusion_mode
        
        # é‡æ„ bbox_util ä»¥åŒ¹é… Fusion æ¨¡å‹çš„è°ƒç”¨æ–¹å¼
        self.bbox_util = DecodeBox(self.anchors, self.num_classes, 
                                   (self.input_shape[0], self.input_shape[1]), 
                                   self.anchors_mask)
        
        print(f"âœ“ FusionCocoEvalCallback åˆå§‹åŒ–å®Œæˆ")
        print(f"  - RGB å›¾ç‰‡ç›®å½•: {self.image_dir_rgb}")
        print(f"  - Event å›¾ç‰‡ç›®å½•: {self.image_dir_event}")
        print(f"  - Fusion æ¨¡å¼: {self.fusion_mode}")
    
    def _prepare_fusion_inputs(self, image_data: np.ndarray, image_id: str):
        """
        å‡†å¤‡ Fusion æ¨¡å‹çš„åŒæ¨¡æ€è¾“å…¥
        
        Args:
            image_data: RGB å›¾ç‰‡æ•°æ®ï¼ˆç»è¿‡é¢„å¤„ç†ï¼‰
            image_id: å›¾ç‰‡ IDï¼Œç”¨äºç»„è£… Event å›¾ç‰‡è·¯å¾„
            
        Returns:
            rgb_images: RGB æ¨¡æ€ tensor [B, C, H, W]
            event_images: Event æ¨¡æ€ tensor [B, C, H, W]
        """
        model_device = next(self.net.parameters()).device
        
        if self.fusion_mode == "rgb_only":
            # åªä½¿ç”¨ RGB æ¨¡æ€ï¼ŒEvent æ¨¡æ€ç”¨ RGB å¡«å……
            rgb_tensor = torch.from_numpy(image_data).to(model_device, non_blocking=True)
            event_tensor = rgb_tensor.clone()
            return rgb_tensor, event_tensor
        
        elif self.fusion_mode == "event_only":
            # åªä½¿ç”¨ Event æ¨¡æ€ï¼ŒRGB æ¨¡æ€ç”¨ Event å¡«å……
            # éœ€è¦ä» Event å›¾ç‰‡ç›®å½•åŠ è½½å›¾ç‰‡
            event_path = os.path.join(self.image_dir_event, f"{image_id}.png")
            if not os.path.exists(event_path):
                # å¦‚æœæ‰¾ä¸åˆ° Event å›¾ç‰‡ï¼Œå›é€€åˆ° RGB
                event_path = os.path.join(self.image_dir_event, f"{image_id}.jpg")
            
            if os.path.exists(event_path):
                event_image = Image.open(event_path)
                event_data = self._preprocess_image(event_image)
                event_tensor = torch.from_numpy(event_data).to(model_device, non_blocking=True)
            else:
                # æ‰¾ä¸åˆ° Event å›¾ç‰‡ï¼Œç”¨ RGB å¡«å……
                event_tensor = torch.from_numpy(image_data).to(model_device, non_blocking=True)
            
            rgb_tensor = event_tensor.clone()
            return rgb_tensor, event_tensor
        
        elif self.fusion_mode == "dual_avg":
            # ä½¿ç”¨åŒæ¨¡æ€å¹³å‡å€¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
            rgb_tensor = torch.from_numpy(image_data).to(model_device, non_blocking=True)
            
            # å°è¯•åŠ è½½ Event å›¾ç‰‡
            event_path = os.path.join(self.image_dir_event, f"{image_id}.png")
            if not os.path.exists(event_path):
                event_path = os.path.join(self.image_dir_event, f"{image_id}.jpg")
            
            if os.path.exists(event_path):
                event_image = Image.open(event_path)
                event_data = self._preprocess_image(event_image)
                event_tensor = torch.from_numpy(event_data).to(model_device, non_blocking=True)
            else:
                event_tensor = rgb_tensor.clone()
            
            return rgb_tensor, event_tensor
        
        elif self.fusion_mode == "dual_concat":
            # åŒæ¨¡æ€æ‹¼æ¥ï¼ˆéœ€è¦ Fusion æ¨¡å‹æ”¯æŒï¼‰
            rgb_tensor = torch.from_numpy(image_data).to(model_device, non_blocking=True)
            
            # å°è¯•åŠ è½½ Event å›¾ç‰‡
            event_path = os.path.join(self.image_dir_event, f"{image_id}.png")
            if not os.path.exists(event_path):
                event_path = os.path.join(self.image_dir_event, f"{image_id}.jpg")
            
            if os.path.exists(event_path):
                event_image = Image.open(event_path)
                event_data = self._preprocess_image(event_image)
                event_tensor = torch.from_numpy(event_data).to(model_device, non_blocking=True)
            else:
                event_tensor = rgb_tensor.clone()
            
            return rgb_tensor, event_tensor
        
        else:
            # é»˜è®¤è¡Œä¸ºï¼šRGB only
            rgb_tensor = torch.from_numpy(image_data).to(model_device, non_blocking=True)
            event_tensor = rgb_tensor.clone()
            return rgb_tensor, event_tensor
    
    def _preprocess_image(self, image):
        """é¢„å¤„ç†å•å¼ å›¾ç‰‡"""
        image = cvtColor(image)
        image_data = resize_image(image, (self.input_shape[1], self.input_shape[0]), 
                                 self.letterbox_image)
        image_data = np.expand_dims(np.transpose(preprocess_input(
            np.array(image_data, dtype='float32')), (2, 0, 1)), 0)
        return image_data
    
    def get_map_txt(self, image_id, image, class_names, map_out_path):
        """
        ç”Ÿæˆé¢„æµ‹ç»“æœ txt æ–‡ä»¶ï¼ˆæ”¯æŒ Fusion æ¨¡å‹ï¼‰
        
        Args:
            image_id: å›¾ç‰‡ ID
            image: PIL Image å¯¹è±¡ï¼ˆRGBï¼‰
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            map_out_path: è¾“å‡ºè·¯å¾„
        """
        f = open(os.path.join(map_out_path, "detection-results/" + image_id + ".txt"), 
                "w", encoding='utf-8')
        
        try:
            # é¢„å¤„ç† RGB å›¾ç‰‡
            image_shape = np.array(np.shape(image)[0:2])
            image_data = self._preprocess_image(image)
            
            # å‡†å¤‡ Fusion æ¨¡å‹è¾“å…¥
            rgb_images, event_images = self._prepare_fusion_inputs(image_data, image_id)
            
            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†ï¼ˆå¦‚æœ CUDA å¯ç”¨ï¼‰
            with torch.cuda.amp.autocast(enabled=self.cuda and rgb_images.device.type == 'cuda'):
                with torch.no_grad():
                    # Fusion æ¨¡å‹å‰å‘ä¼ æ’­
                    outputs = self.net(rgb_images, event_images)
            
            # è§£ç å’Œ NMS
            outputs = self.bbox_util.decode_box(outputs)
            results = self.bbox_util.non_max_suppression(
                torch.cat(outputs, 1), self.num_classes, self.input_shape,
                image_shape, self.letterbox_image, 
                conf_thres=self.confidence, nms_thres=self.nms_iou
            )
            
            if results[0] is None:
                f.close()
                if self.cuda:
                    torch.cuda.empty_cache()
                return
            
            top_label = np.array(results[0][:, 6], dtype='int32')
            top_confidence = results[0][:, 4] * results[0][:, 5]
            top_boxes = results[0][:, :4]
            
            # åªä¿ç•™ top-N ä¸ªç»“æœ
            top_indices = np.argsort(top_confidence)[::-1][:self.max_boxes]
            top_boxes = top_boxes[top_indices]
            top_confidence = top_confidence[top_indices]
            top_label = top_label[top_indices]
            
            for i, c in list(enumerate(top_label)):
                predicted_class = class_names[int(c)]
                box = top_boxes[i]
                score = str(top_confidence[i])
                
                top, left, bottom, right = box
                if predicted_class not in class_names:
                    continue
                
                f.write("%s %s %s %s %s %s\n" % (
                    predicted_class, score[:6], 
                    str(int(left)), str(int(top)), 
                    str(int(right)), str(int(bottom))
                ))
            
            f.close()
            
            # æ¸…ç†æ˜¾å­˜
            if self.cuda:
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âš ï¸  - è¯„ä¼°å›¾ç‰‡ {image_id} å¤±è´¥: {e}")
            f.close()
            if self.cuda:
                torch.cuda.empty_cache()
    
    def on_epoch_end(self, epoch, model_eval):
        """Epoch ç»“æŸæ—¶çš„å›è°ƒ"""
        if epoch % self.period != 0 or not self.eval_flag:
            return
        
        self.net = model_eval
        self.net.eval()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.map_out_path, exist_ok=True)
        os.makedirs(os.path.join(self.map_out_path, "ground-truth"), exist_ok=True)
        os.makedirs(os.path.join(self.map_out_path, "detection-results"), exist_ok=True)
        
        print(f"\n{'='*60}")
        print(f"Fusion æ¨¡å‹è¯„ä¼° (Epoch {epoch})")
        print(f"è¯„æµ‹æ¨¡å¼: {self.fusion_mode}")
        print(f"{'='*60}")
        
        # å¿«é€ŸéªŒè¯æ¨¡å¼
        eval_images = self.images
        if self.max_eval_samples is not None:
            eval_images = self.images[:self.max_eval_samples]
            print(f"âš¡ å¿«é€ŸéªŒè¯æ¨¡å¼: ä»…è¯„ä¼° {len(eval_images)} ä¸ªæ ·æœ¬ï¼ˆå…± {len(self.images)} ä¸ªï¼‰")
        
        # éå†éªŒè¯é›†
        for img_info in tqdm(eval_images, desc="è¯„ä¼°", unit="img"):
            file_name = img_info.get('file_name') or img_info.get('rgb_file_name')
            if not file_name:
                continue
            
            # æå–å›¾ç‰‡ ID
            image_id = os.path.splitext(os.path.basename(file_name))[0]
            img_path = os.path.join(self.image_dir_rgb, file_name)
            
            if not os.path.exists(img_path):
                continue
            
            try:
                # è¯»å– RGB å›¾ç‰‡
                image = Image.open(img_path)
                
                # ç”Ÿæˆé¢„æµ‹ç»“æœ
                self.get_map_txt(image_id, image, self.class_names, self.map_out_path)
                
                # ç”ŸæˆçœŸå®æ¡† txt
                gt_path = os.path.join(self.map_out_path, "ground-truth/" + image_id + ".txt")
                with open(gt_path, "w", encoding='utf-8') as gt_f:
                    if img_info['id'] in self.img_to_anns:
                        for ann in self.img_to_anns[img_info['id']]:
                            bbox = ann['bbox']  # [x, y, width, height]
                            
                            # è½¬æ¢ä¸º [left, top, right, bottom]
                            left = int(bbox[0])
                            top = int(bbox[1])
                            right = int(bbox[0] + bbox[2])
                            bottom = int(bbox[1] + bbox[3])
                            
                            # è·å–ç±»åˆ«
                            class_idx = self.cat_id_to_idx[ann['category_id']]
                            obj_name = self.class_names[class_idx]
                            
                            gt_f.write("%s %s %s %s %s\n" % (
                                obj_name, left, top, right, bottom
                            ))
            
            except Exception as e:
                print(f"âš ï¸  - å¤„ç†å›¾ç‰‡ {image_id} å¤±è´¥: {e}")
                continue
        
        # è®¡ç®— mAP
        print("è®¡ç®— mAP...")
        try:
            from .utils_map import get_coco_map
            temp_map = get_coco_map(class_names=self.class_names, path=self.map_out_path)[1]
            temp_map = float(temp_map) if isinstance(temp_map, (int, float)) else 0.0
        except Exception as e:
            print(f"âš ï¸  - COCO mAP è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ VOC æ–¹å¼: {e}")
            from .utils_map import get_map
            temp_map = get_map(self.MINOVERLAP, False, path=self.map_out_path)
        
        self.maps.append(temp_map)
        self.epoches.append(epoch)
        
        # ä¿å­˜ mAP è®°å½•
        map_file = os.path.join(self.log_dir, "epoch_map.txt")
        with open(map_file, 'a') as f:
            f.write(str(temp_map))
            f.write("\n")
        
        # ç»˜åˆ¶ mAP æ›²çº¿
        plt.figure(figsize=(10, 6))
        plt.plot(self.epoches, self.maps, 'red', linewidth=2, label='Validation mAP')
        
        plt.grid(True, alpha=0.3)
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel(f'mAP@{self.MINOVERLAP}', fontsize=12)
        plt.title('Fusion Model mAP Curve', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.ylim(0, 1)
        
        plt.savefig(os.path.join(self.log_dir, "epoch_map.png"), 
                   dpi=150, bbox_inches='tight')
        plt.close()
        
        # è®¡ç®—æœ€ä½³ mAP
        if temp_map > max(self.maps[:-1], default=0):
            print(f"ğŸ‰  æ–°æœ€ä½³ mAP: {temp_map:.4f}")
        
        print(f"\nEpoch {epoch} ç»“æœ:")
        print(f"  - mAP@{self.MINOVERLAP}: {temp_map:.4f}")
        print(f"  - å½“å‰æœ€ä½³: {max(self.maps):.4f}")
        print(f"  - è¯„ä¼°æ ·æœ¬: {len(eval_images)}/{len(self.images)}")
        print(f"  - Fusion æ¨¡å¼: {self.fusion_mode}")
        print(f"{'='*60}\n")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            shutil.rmtree(self.map_out_path, ignore_errors=True)
        except:
            pass


class FusionSimplifiedEvalCallback:
    """
    ç®€åŒ–ç‰ˆ Fusion è¯„ä¼°å›è°ƒ
    åªè®¡ç®—éªŒè¯é›† lossï¼Œä¸è®¡ç®— mAPï¼ˆé€‚åˆå¿«é€Ÿè®­ç»ƒï¼‰
    """
    def __init__(self, log_dir, eval_flag=True, period=1):
        """
        Args:
            log_dir: æ—¥å¿—ç›®å½•
            eval_flag: æ˜¯å¦è¿›è¡Œè¯„ä¼°
            period: è¯„ä¼°å‘¨æœŸ
        """
        self.log_dir = log_dir
        self.eval_flag = eval_flag
        self.period = period
        
        self.epoches = [0]
        
        if self.eval_flag:
            os.makedirs(self.log_dir, exist_ok=True)
    
    def on_epoch_end(self, epoch, model_eval):
        """Epoch ç»“æŸæ—¶çš„å›è°ƒ"""
        if epoch % self.period == 0 and self.eval_flag:
            self.epoches.append(epoch)
            print(f"âœ“ Epoch {epoch} å®Œæˆ (ç®€åŒ–è¯„ä¼°)")