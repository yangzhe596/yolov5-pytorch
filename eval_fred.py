#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹å·²è®­ç»ƒçš„FREDæ¨¡å‹è¿›è¡ŒmAPè¯„ä¼°
"""
import os
import json
import argparse
from tqdm import tqdm

import numpy as np
import torch
from PIL import Image

from nets.yolo import YoloBody
from utils.utils import get_anchors, cvtColor, preprocess_input, resize_image
from utils.utils_bbox import DecodeBox
from utils.utils_map import get_coco_map, get_map

# å¯¼å…¥FREDé…ç½®
import config_fred as cfg


def evaluate_model(model_path, modality='rgb', 
                   confidence=None, nms_iou=None, input_shape=None,
                   map_out_path='map_out', MINOVERLAP=None,
                   high_res=False, four_features=False):
    """
    è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        modality: æ¨¡æ€ (rgb/event)
        confidence: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        nms_iou: NMS IOUé˜ˆå€¼ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        input_shape: è¾“å…¥å°ºå¯¸ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        map_out_path: mAPè®¡ç®—è¾“å‡ºç›®å½•
        MINOVERLAP: mAPè®¡ç®—çš„IOUé˜ˆå€¼ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        high_res: æ˜¯å¦ä½¿ç”¨é«˜åˆ†è¾¨ç‡æ¨¡å¼
        four_features: æ˜¯å¦ä½¿ç”¨å››ç‰¹å¾å±‚æ¨¡å¼ï¼ˆéœ€è¦high_res=Trueï¼‰
    """
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if confidence is None:
        confidence = cfg.CONFIDENCE
    if nms_iou is None:
        nms_iou = cfg.NMS_IOU
    if input_shape is None:
        input_shape = tuple(cfg.INPUT_SHAPE)
    if MINOVERLAP is None:
        MINOVERLAP = cfg.MINOVERLAP
    
    # è®¾å¤‡é…ç½®
    cuda = cfg.CUDA and torch.cuda.is_available()
    device = torch.device('cuda' if cuda else 'cpu')
    
    # æ•°æ®é›†é…ç½®
    test_json = cfg.get_annotation_path(modality, 'test')
    test_img_dir = cfg.get_image_dir(modality)
    
    if not os.path.exists(test_json):
        raise FileNotFoundError(f"æµ‹è¯•é›†æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {test_json}")
    
    # æ¨¡å‹é…ç½®
    num_classes = cfg.NUM_CLASSES
    class_names = cfg.CLASS_NAMES
    anchors_path = cfg.ANCHORS_PATH
    anchors_mask = cfg.ANCHORS_MASK
    backbone = cfg.BACKBONE
    phi = cfg.PHI
    
    print("=" * 70)
    print(f"FRED {modality.upper()} æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    print(f"æ¨¡å‹æƒé‡: {model_path}")
    print(f"æµ‹è¯•é›†: {test_json}")
    print(f"è¾“å…¥å°ºå¯¸: {input_shape}")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {confidence}")
    print(f"NMS IOUé˜ˆå€¼: {nms_iou}")
    print(f"mAP IOUé˜ˆå€¼: {MINOVERLAP}")
    print("=" * 70)
    
    # åŠ è½½å…ˆéªŒæ¡†
    anchors, num_anchors = get_anchors(anchors_path)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    model = YoloBody(anchors_mask, num_classes, phi, backbone, 
                     pretrained=False, input_shape=input_shape,
                     high_res=high_res, four_features=four_features)
    
    # åŠ è½½æƒé‡
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    model_dict = model.state_dict()
    pretrained_dict = torch.load(model_path, map_location=device)
    
    # å¤„ç†å¯èƒ½çš„DataParallelåŒ…è£…
    if list(pretrained_dict.keys())[0].startswith('module.'):
        pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items()}
    
    load_key, no_load_key, temp_dict = [], [], {}
    for k, v in pretrained_dict.items():
        if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):
            temp_dict[k] = v
            load_key.append(k)
        else:
            no_load_key.append(k)
    
    model_dict.update(temp_dict)
    model.load_state_dict(model_dict)
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(load_key)} ä¸ªå‚æ•°")
    if len(no_load_key) > 0:
        print(f"âš  æœªåŠ è½½ {len(no_load_key)} ä¸ªå‚æ•°")
    
    model.eval()
    if cuda:
        model = model.cuda()
    
    # åˆ›å»ºè§£ç å™¨
    bbox_util = DecodeBox(anchors, num_classes, 
                         (input_shape[0], input_shape[1]), anchors_mask, 
                         high_res=high_res, four_features=four_features)
    
    # åŠ è½½COCOæµ‹è¯•é›†
    print("\nåŠ è½½æµ‹è¯•é›†...")
    with open(test_json, 'r') as f:
        coco_data = json.load(f)
    
    # åˆ›å»ºcategory_idåˆ°ç´¢å¼•çš„æ˜ å°„
    cat_id_to_idx = {}
    for cat in coco_data['categories']:
        cat_id_to_idx[cat['id']] = cat['id'] - 1
    
    # åˆ›å»ºimage_idåˆ°annotationsçš„æ˜ å°„
    img_to_anns = {}
    for ann in coco_data['annotations']:
        img_id = ann['image_id']
        if img_id not in img_to_anns:
            img_to_anns[img_id] = []
        img_to_anns[img_id].append(ann)
    
    images = coco_data['images']
    print(f"âœ“ æµ‹è¯•é›†: {len(images)} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(map_out_path, exist_ok=True)
    os.makedirs(os.path.join(map_out_path, "ground-truth"), exist_ok=True)
    os.makedirs(os.path.join(map_out_path, "detection-results"), exist_ok=True)
    
    print("\nå¼€å§‹è¯„ä¼°...")
    
    # ç»Ÿè®¡ç¼ºå¤±å›¾ç‰‡
    missing_images = 0
    
    # éå†æµ‹è¯•é›†
    for img_info in tqdm(images, desc="å¤„ç†å›¾ç‰‡"):
        # åªä½¿ç”¨æ–‡ä»¶åï¼ˆä¸åŒ…å«ç›®å½•ï¼‰ï¼Œé¿å…è·¯å¾„é—®é¢˜
        file_name = img_info['file_name']
        image_id = os.path.splitext(os.path.basename(file_name))[0]
        img_path = os.path.join(test_img_dir, file_name)
        
        if not os.path.exists(img_path):
            missing_images += 1
            continue
        
        # è¯»å–å›¾åƒ
        image = Image.open(img_path)
        image_shape = np.array(np.shape(image)[0:2])
        
        # é¢„å¤„ç†
        image_rgb = cvtColor(image)
        image_data = resize_image(image_rgb, (input_shape[1], input_shape[0]), True)
        image_data = np.expand_dims(
            np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0
        )
        
        # é¢„æµ‹
        with torch.no_grad():
            images_tensor = torch.from_numpy(image_data)
            if cuda:
                images_tensor = images_tensor.cuda()
            
            outputs = model(images_tensor)
            outputs = bbox_util.decode_box(outputs)
            results = bbox_util.non_max_suppression(
                torch.cat(outputs, 1), num_classes, input_shape,
                image_shape, True, conf_thres=confidence, nms_thres=nms_iou
            )
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        det_file = os.path.join(map_out_path, "detection-results", f"{image_id}.txt")
        with open(det_file, "w", encoding='utf-8') as f:
            if results[0] is not None:
                top_label = np.array(results[0][:, 6], dtype='int32')
                top_conf = results[0][:, 4] * results[0][:, 5]
                top_boxes = results[0][:, :4]
                
                for i, c in enumerate(top_label):
                    predicted_class = class_names[int(c)]
                    box = top_boxes[i]
                    score = top_conf[i]
                    
                    top, left, bottom, right = box
                    f.write(f"{predicted_class} {score:.6f} {int(left)} {int(top)} {int(right)} {int(bottom)}\n")
        
        # ä¿å­˜çœŸå®æ¡†
        gt_file = os.path.join(map_out_path, "ground-truth", f"{image_id}.txt")
        with open(gt_file, "w", encoding='utf-8') as f:
            if img_info['id'] in img_to_anns:
                for ann in img_to_anns[img_info['id']]:
                    bbox = ann['bbox']  # [x, y, width, height]
                    
                    # è½¬æ¢ä¸º[left, top, right, bottom]
                    left = int(bbox[0])
                    top = int(bbox[1])
                    right = int(bbox[0] + bbox[2])
                    bottom = int(bbox[1] + bbox[3])
                    
                    # è·å–ç±»åˆ«
                    class_idx = cat_id_to_idx[ann['category_id']]
                    obj_name = class_names[class_idx]
                    
                    f.write(f"{obj_name} {left} {top} {right} {bottom}\n")
    
    # æŠ¥å‘Šç¼ºå¤±å›¾ç‰‡
    if missing_images > 0:
        print(f"\nâš  è­¦å‘Š: {missing_images} å¼ å›¾ç‰‡æœªæ‰¾åˆ°")
    
    # è®¡ç®—mAP
    print("\nè®¡ç®—mAP...")
    # é¦–å…ˆæ£€æŸ¥pycocotoolsæ˜¯å¦å¯ç”¨
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval
        # å¦‚æœå¯¼å…¥æˆåŠŸï¼Œå°è¯•ä½¿ç”¨COCO API
        temp_map = get_coco_map(class_names=class_names, path=map_out_path)[1]
        print(f"\n{'='*70}")
        print(f"mAP@{MINOVERLAP} (COCO): {temp_map:.4f}")
        print(f"{'='*70}")
    except (ImportError, NameError, Exception) as e:
        # ä½¿ç”¨VOCæ–¹å¼è®¡ç®—
        print(f"ä½¿ç”¨VOCæ–¹å¼è®¡ç®—mAP (COCO APIä¸å¯ç”¨: {str(e)})")
        temp_map = get_map(MINOVERLAP, False, path=map_out_path)
        print(f"\n{'='*70}")
        print(f"mAP@{MINOVERLAP} (VOC): {temp_map:.4f}")
        print(f"{'='*70}")
    
    # ä¿å­˜ç»“æœ
    save_dir = cfg.get_save_dir(modality)
    result_file = os.path.join(save_dir, f"eval_result_{modality}.txt")
    with open(result_file, 'w') as f:
        f.write(f"Model: {model_path}\n")
        f.write(f"Test Set: {test_json}\n")
        f.write(f"mAP@{MINOVERLAP}: {temp_map:.4f}\n")
    
    print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    return temp_map


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è¯„ä¼°FREDè®­ç»ƒçš„YOLOv5æ¨¡å‹')
    parser.add_argument('--model_path', type=str, default='',
                        help='æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€ä½³æƒé‡ï¼‰')
    parser.add_argument('--modality', type=str, default='rgb', choices=['rgb', 'event'],
                        help='æ¨¡æ€: rgb æˆ– event')
    parser.add_argument('--confidence', type=float, default=None,
                        help=f'ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: {cfg.CONFIDENCE}ï¼‰')
    parser.add_argument('--nms_iou', type=float, default=None,
                        help=f'NMS IOUé˜ˆå€¼ï¼ˆé»˜è®¤: {cfg.NMS_IOU}ï¼‰')
    parser.add_argument('--input_shape', type=int, nargs=2, default=None,
                        help=f'è¾“å…¥å°ºå¯¸ (height width)ï¼ˆé»˜è®¤: {cfg.INPUT_SHAPE}ï¼‰')
    parser.add_argument('--map_out_path', type=str, default='map_out',
                        help='mAPè®¡ç®—è¾“å‡ºç›®å½•')
    parser.add_argument('--minoverlap', type=float, default=None,
                        help=f'mAPè®¡ç®—çš„IOUé˜ˆå€¼ï¼ˆé»˜è®¤: {cfg.MINOVERLAP}ï¼‰')
    parser.add_argument('--high_res', action='store_true',
                        help='ä½¿ç”¨é«˜åˆ†è¾¨ç‡æ¨¡å¼æ¨¡å‹')
    parser.add_argument('--four_features', action='store_true',
                        help='ä½¿ç”¨å››ç‰¹å¾å±‚æ¨¡å¼æ¨¡å‹ï¼ˆéœ€è¦åŒæ—¶æŒ‡å®š--high_resï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®é«˜åˆ†è¾¨ç‡æ¨¡å¼
    if args.high_res:
        cfg.configure_high_res_mode(True, args.four_features)
        print(f"\n{'='*70}")
        if args.four_features:
            print("ğŸ” å››ç‰¹å¾å±‚é«˜åˆ†è¾¨ç‡æ¨¡å¼å·²å¯ç”¨")
            print("  - ç‰¹å¾å±‚: 160x160, 80x80, 40x40, 20x20")
        else:
            print("ğŸ” é«˜åˆ†è¾¨ç‡æ¨¡å¼å·²å¯ç”¨")
            print("  - ç‰¹å¾å±‚: 160x160, 80x80, 40x40")
        print(f"{'='*70}\n")
    
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€ä½³æƒé‡
    if not args.model_path:
        args.model_path = cfg.get_model_path(args.modality, best=True)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {args.model_path}")
        print(f"\nè¯·å…ˆè®­ç»ƒæ¨¡å‹:")
        if args.four_features:
            print(f"  python train_fred.py --modality {args.modality} --high_res --four_features")
        elif args.high_res:
            print(f"  python train_fred.py --modality {args.modality} --high_res")
        else:
            print(f"  python train_fred.py --modality {args.modality}")
        exit(1)
    
    # è½¬æ¢input_shapeä¸ºtuple
    input_shape = tuple(args.input_shape) if args.input_shape else None
    
    evaluate_model(
        model_path=args.model_path,
        modality=args.modality,
        confidence=args.confidence,
        nms_iou=args.nms_iou,
        input_shape=input_shape,
        map_out_path=args.map_out_path,
        MINOVERLAP=args.minoverlap,
        high_res=args.high_res,
        four_features=args.four_features
    )
